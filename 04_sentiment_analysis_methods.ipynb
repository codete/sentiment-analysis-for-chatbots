{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis methods\n",
    "\n",
    "There is no ML method which solves specifically the problem of sentiment analysis, so it is important to test some different algorithms and choose the best one. In our tests we are going to use scikit-learn library, which is commonly used, especially for implementing POCs.\n",
    "\n",
    "The quality of the ML model has to be measured somehow in order to compare their efficiency for given problem. In our case we are going to use quite a simple metric, called accuracy. \n",
    "\n",
    "$$accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
    "\n",
    "We have three different labels for our texts: positive, negative and neutral. To avoid dealing with texts, we are going to convert them to 1, -1 and 0, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 02_data_preparation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with defining a base pipeline for our dataset. We're going to use some functions declared in previous parts of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SENTIMENT_TO_LABEL_MAPPING = {\n",
    "    \"negative\": -1,\n",
    "    \"neutral\": 0,\n",
    "    \"positive\": 1\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "raw_tweets = pd.read_csv(\"data/twitter-airlines-sentiment.csv\")\n",
    "\n",
    "# Preprocess the data with the function declared previously\n",
    "tweets = raw_tweets[[\"airline_sentiment\", \"text\"]]\n",
    "tweets.columns = (\"sentiment\", \"text\", )\n",
    "tweets[\"text\"] = tweets[\"text\"].map(preprocess_text)\n",
    "tweets[\"sentiment\"] = tweets[\"sentiment\"].map(lambda x: SENTIMENT_TO_LABEL_MAPPING[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Divide the dataset into train and test fraction\n",
    "train_messages, test_messages, train_targets, test_targets = train_test_split(tweets[\"text\"], \n",
    "                                                                              tweets[\"sentiment\"],\n",
    "                                                                              test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 962 ms, sys: 1.42 s, total: 2.39 s\n",
      "Wall time: 1.36 s\n",
      "CPU times: user 186 ms, sys: 156 ms, total: 342 ms\n",
      "Wall time: 341 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy score:0.6232923497267759\n",
      "\n",
      "CPU times: user 13.7 s, sys: 546 ms, total: 14.3 s\n",
      "Wall time: 14.3 s\n",
      "CPU times: user 13min 13s, sys: 190 ms, total: 13min 13s\n",
      "Wall time: 13min 13s\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy score:0.5140027322404371\n",
      "\n",
      "CPU times: user 11.9 s, sys: 564 ms, total: 12.4 s\n",
      "Wall time: 12.4 s\n",
      "CPU times: user 13min 27s, sys: 152 ms, total: 13min 27s\n",
      "Wall time: 13min 27s\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy score:0.4371584699453552\n",
      "\n",
      "CPU times: user 14.3 s, sys: 652 ms, total: 14.9 s\n",
      "Wall time: 14.9 s\n",
      "CPU times: user 13min 22s, sys: 180 ms, total: 13min 22s\n",
      "Wall time: 13min 22s\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=27, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy score:0.35963114754098363\n",
      "\n",
      "CPU times: user 2h 28min 48s, sys: 1.61 s, total: 2h 28min 49s\n",
      "Wall time: 2h 28min 49s\n",
      "CPU times: user 6min 6s, sys: 156 ms, total: 6min 6s\n",
      "Wall time: 6min 6s\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy score:0.6232923497267759\n",
      "\n",
      "CPU times: user 1.52 s, sys: 520 ms, total: 2.04 s\n",
      "Wall time: 2.04 s\n",
      "CPU times: user 146 ms, sys: 164 ms, total: 310 ms\n",
      "Wall time: 309 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Accuracy score:0.7797131147540983\n",
      "\n",
      "CPU times: user 1min 8s, sys: 428 ms, total: 1min 9s\n",
      "Wall time: 1min 9s\n",
      "CPU times: user 76 ms, sys: 84 ms, total: 160 ms\n",
      "Wall time: 160 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Accuracy score:0.6905737704918032\n",
      "\n",
      "CPU times: user 10.7 s, sys: 388 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n",
      "CPU times: user 106 ms, sys: 88 ms, total: 194 ms\n",
      "Wall time: 193 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=2018, verbose=0,\n",
      "            warm_start=False)\n",
      "Accuracy score:0.7281420765027322\n",
      "\n",
      "CPU times: user 2.01 s, sys: 1.31 s, total: 3.32 s\n",
      "Wall time: 3.32 s\n",
      "CPU times: user 582 ms, sys: 560 ms, total: 1.14 s\n",
      "Wall time: 1.14 s\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Classifier: GaussianNB(priors=None)\n",
      "Accuracy score:0.516051912568306\n",
      "\n",
      "CPU times: user 460 ms, sys: 732 ms, total: 1.19 s\n",
      "Wall time: 665 ms\n",
      "CPU times: user 120 ms, sys: 72 ms, total: 192 ms\n",
      "Wall time: 192 ms\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "Classifier: LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy score:0.6232923497267759\n",
      "\n",
      "CPU times: user 9.54 s, sys: 444 ms, total: 9.99 s\n",
      "Wall time: 9.97 s\n",
      "CPU times: user 12min 5s, sys: 92 ms, total: 12min 6s\n",
      "Wall time: 12min 6s\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy score:0.6844262295081968\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.41 s, sys: 380 ms, total: 9.79 s\n",
      "Wall time: 9.78 s\n",
      "CPU times: user 12min 12s, sys: 88 ms, total: 12min 12s\n",
      "Wall time: 12min 12s\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy score:0.7315573770491803\n",
      "\n",
      "CPU times: user 9.33 s, sys: 424 ms, total: 9.75 s\n",
      "Wall time: 9.73 s\n",
      "CPU times: user 12min 13s, sys: 88 ms, total: 12min 13s\n",
      "Wall time: 12min 13s\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=27, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy score:0.7489754098360656\n",
      "\n",
      "CPU times: user 2h 17min 7s, sys: 1.28 s, total: 2h 17min 9s\n",
      "Wall time: 2h 17min 8s\n",
      "CPU times: user 6min 5s, sys: 80 ms, total: 6min 5s\n",
      "Wall time: 6min 5s\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "Classifier: SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy score:0.6232923497267759\n",
      "\n",
      "CPU times: user 417 ms, sys: 316 ms, total: 733 ms\n",
      "Wall time: 733 ms\n",
      "CPU times: user 88.9 ms, sys: 92 ms, total: 181 ms\n",
      "Wall time: 180 ms\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "Classifier: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Accuracy score:0.7950819672131147\n",
      "\n",
      "CPU times: user 1min 33s, sys: 516 ms, total: 1min 34s\n",
      "Wall time: 1min 34s\n",
      "CPU times: user 64.7 ms, sys: 128 ms, total: 193 ms\n",
      "Wall time: 192 ms\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Accuracy score:0.6594945355191257\n",
      "\n",
      "CPU times: user 11.7 s, sys: 476 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n",
      "CPU times: user 122 ms, sys: 96 ms, total: 218 ms\n",
      "Wall time: 217 ms\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "Classifier: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=2018, verbose=0,\n",
      "            warm_start=False)\n",
      "Accuracy score:0.7295081967213115\n",
      "\n",
      "CPU times: user 1.34 s, sys: 1.4 s, total: 2.74 s\n",
      "Wall time: 2.74 s\n",
      "CPU times: user 467 ms, sys: 564 ms, total: 1.03 s\n",
      "Wall time: 1.03 s\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "Classifier: GaussianNB(priors=None)\n",
      "Accuracy score:0.5129781420765027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Declare vectorizers to be used\n",
    "VECTORIZERS = (\n",
    "    CountVectorizer(),\n",
    "    TfidfVectorizer(),\n",
    ")\n",
    "\n",
    "# Declare classifiers to be used\n",
    "CLASSIFIERS = (\n",
    "    LogisticRegression(C=10e-5, solver=\"liblinear\", max_iter=10000),\n",
    "    KNeighborsClassifier(3),\n",
    "    KNeighborsClassifier(9),\n",
    "    KNeighborsClassifier(27),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    LinearSVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(random_state=2018),\n",
    "    GaussianNB(),\n",
    ")\n",
    "\n",
    "for vectorizer, classifier in itertools.product(VECTORIZERS, CLASSIFIERS):\n",
    "    # Vectorize preprocessed sentences\n",
    "    train_features = vectorizer.fit_transform(train_messages)\n",
    "\n",
    "    # Train the model\n",
    "    %time fit = classifier.fit(train_features.toarray(), train_targets)\n",
    "\n",
    "    # Check the accuracy of the model on test data and display it\n",
    "    test_features = vectorizer.transform(test_messages)\n",
    "    %time test_predictions = fit.predict(test_features.toarray())\n",
    "    accuracy = accuracy_score(test_predictions, test_targets)\n",
    "    print(\"Vectorizer: {}\\nClassifier: {}\\nAccuracy score:{}\\n\".format(vectorizer,\n",
    "                                                                       classifier, \n",
    "                                                                       accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
