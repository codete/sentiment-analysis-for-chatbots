{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest for sentiment analysis\n",
    "\n",
    "In the previous chapter we saw one of the most promising models was based on Random Forest. It combines quite a good accuracy with a performance. It is really important if we'd like to move our model to production, so the best architecture, in terms of its precision, is not always a possible choice. \n",
    "\n",
    "We are going to begin with a simple description of the Random Forest, to understand how it works under the hood. In simple words, Random Forest is a collection of ensembled Decision Trees which vote in order to form a single decision of belonging to a particular class. Each Decision Tree uses a randomly selected subset of features in order to perform its own decision.\n",
    "\n",
    "## Tuning the model parameters\n",
    "\n",
    "Let's play a little bit with the hyperparameters of the Random Forest Classifier, to optimize its accuracy. First of all, let's list some of the possible parameters and their values: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "From our perspective the following parameters look like the ones we should test out:\n",
    "\n",
    "- **n_estimators** - number of decision trees used to make a forest, by default set to 10\n",
    "- **criterion** - quality function for measuring a split, can be set to \"gini\" (default) or \"entropy\"\n",
    "- **max_features** - a maximum number of features to consider (int - exact number, float - percentage, \"auto\", \"sqrt\", \"log2\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.59 s, sys: 496 ms, total: 5.09 s\n",
      "Wall time: 5.09 s\n",
      "Configuration: n_estimators = 5, criterion = gini, max_features = auto\n",
      "Train accuracy score: 0.9565403005464481\n",
      "Test accuracy score: 0.7131147540983607\n",
      "\n",
      "CPU times: user 2.74 s, sys: 496 ms, total: 3.23 s\n",
      "Wall time: 3.23 s\n",
      "Configuration: n_estimators = 5, criterion = gini, max_features = log2\n",
      "Train accuracy score: 0.9636270491803278\n",
      "Test accuracy score: 0.7114071038251366\n",
      "\n",
      "CPU times: user 2min 5s, sys: 555 ms, total: 2min 6s\n",
      "Wall time: 2min 6s\n",
      "Configuration: n_estimators = 5, criterion = gini, max_features = None\n",
      "Train accuracy score: 0.9553449453551912\n",
      "Test accuracy score: 0.7243852459016393\n",
      "\n",
      "CPU times: user 4.03 s, sys: 488 ms, total: 4.51 s\n",
      "Wall time: 4.52 s\n",
      "Configuration: n_estimators = 5, criterion = entropy, max_features = auto\n",
      "Train accuracy score: 0.9609801912568307\n",
      "Test accuracy score: 0.7134562841530054\n",
      "\n",
      "CPU times: user 2.8 s, sys: 564 ms, total: 3.37 s\n",
      "Wall time: 3.37 s\n",
      "Configuration: n_estimators = 5, criterion = entropy, max_features = log2\n",
      "Train accuracy score: 0.9618340163934426\n",
      "Test accuracy score: 0.7056010928961749\n",
      "\n",
      "CPU times: user 1min 34s, sys: 539 ms, total: 1min 35s\n",
      "Wall time: 1min 35s\n",
      "Configuration: n_estimators = 5, criterion = entropy, max_features = None\n",
      "Train accuracy score: 0.9573941256830601\n",
      "Test accuracy score: 0.7192622950819673\n",
      "\n",
      "CPU times: user 8.14 s, sys: 488 ms, total: 8.62 s\n",
      "Wall time: 8.62 s\n",
      "Configuration: n_estimators = 10, criterion = gini, max_features = auto\n",
      "Train accuracy score: 0.9789959016393442\n",
      "Test accuracy score: 0.7336065573770492\n",
      "\n",
      "CPU times: user 4.35 s, sys: 508 ms, total: 4.86 s\n",
      "Wall time: 4.86 s\n",
      "Configuration: n_estimators = 10, criterion = gini, max_features = log2\n",
      "Train accuracy score: 0.9844603825136612\n",
      "Test accuracy score: 0.730191256830601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run 02_data_preparation.ipynb\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "N_ESTIMATORS = (5, 10, 25, 50, 100)\n",
    "CRITERION = (\"gini\", \"entropy\")\n",
    "MAX_FEATURES = (\"auto\", \"log2\", None)\n",
    "\n",
    "# Divide the dataset into train and test fraction\n",
    "train_messages, test_messages, train_targets, test_targets = train_test_split(tweets[\"text\"], \n",
    "                                                                              tweets[\"sentiment\"],\n",
    "                                                                              test_size=0.2)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "for n_estimators, criterion, max_features in itertools.product(N_ESTIMATORS,\n",
    "                                                               CRITERION,\n",
    "                                                               MAX_FEATURES):\n",
    "    # Define the classifier instance\n",
    "    classifier = RandomForestClassifier(random_state=2018, \n",
    "                                        n_estimators=n_estimators, \n",
    "                                        criterion=criterion, \n",
    "                                        max_features=max_features)\n",
    "    # Vectorize preprocessed sentences\n",
    "    train_features = vectorizer.fit_transform(train_messages)\n",
    "\n",
    "    # Train the model\n",
    "    %time fit = classifier.fit(train_features.toarray(), train_targets)\n",
    "\n",
    "    # Check the accuracy of the model on test data and display it\n",
    "    test_features = vectorizer.transform(test_messages)\n",
    "    train_predictions = fit.predict(train_features.toarray())\n",
    "    train_accuracy = accuracy_score(train_predictions, train_targets)\n",
    "    test_predictions = fit.predict(test_features.toarray())\n",
    "    test_accuracy = accuracy_score(test_predictions, test_targets)\n",
    "    print(\"Configuration: n_estimators = {}, criterion = {}, max_features = {}\\n\"\n",
    "          \"Train accuracy score: {}\\n\"\n",
    "          \"Test accuracy score: {}\\n\".format(n_estimators, criterion, max_features, \n",
    "                                             train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned out the following configuration achieves the best accuracy on our test dataset:\n",
    "\n",
    "`n_estimators = 100, criterion = gini, max_features = auto`\n",
    "\n",
    "For that reason we are going to create a simple application that will use these parameters for training. That will be a console application reading the sentences from the user and classifies its sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "Please fill the gaps in the source code in *exercise/exercise_03.py*, in order to create an application that will:\n",
    "\n",
    "- create Random Forest based model and train it on a whole dataset used in previous examples, using selected parameters (`n_estimators = 100, criterion = gini, max_features = auto`)\n",
    "- continiously read the sentences from the standard input and classify them with the created model, until \"exit\" sentence is passed\n",
    "- display the probabilities of beloning to each class\n",
    "\n",
    "Some of the functionalities are already prepared - please complete the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run exercise/exercise_03.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
